{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9ae13f0-51cd-464f-b586-9434db1e8896",
   "metadata": {},
   "source": [
    "# Text mining : Extracting keywords from papers\n",
    "## Abstract\n",
    "Text mining consists in extractings useful information from text data.\n",
    "\n",
    "We will first reproduce the text mining used by VOSviewer, a popular tool for analysing bibliography networks (https://arxiv.org/pdf/1109.2058).\n",
    "\n",
    "Here are their steps related to text mining :\n",
    "<blockquote cite=\"http://www.worldwildlife.org/who/index.html\">\n",
    "    \n",
    "1. <strong>Identification of noun phrases.</strong> The approach that we take is similar to what is\n",
    "reported in an earlier paper (Van Eck, Waltman, Noyons, & Buter, 2010). We first\n",
    "perform part-of-speech tagging (i.e., identification of verbs, nouns, adjectives, etc.).\n",
    "The Apache OpenNLP toolkit (http://incubator.apache.org/opennlp/) is used for this\n",
    "purpose. We then use a linguistic filter to identify noun phrases. Our filter selects all\n",
    "word sequences that consist exclusively of nouns and adjectives and that end with a\n",
    "noun (e.g., paper, visualization, interesting result, and text mining, but not degrees of\n",
    "freedom and highly cited publication). Finally, we convert plural noun phrases into\n",
    "singular ones.\n",
    "\n",
    "2. <p>\n",
    "        <strong>Selection of the most relevant noun phrases.</strong> The selected noun phrases are referred to\n",
    "        as terms. We have developed a new technique for selecting the most relevant noun\n",
    "        phrases. The essence of this technique is as follows. For each noun phrase, the\n",
    "        distribution of (second-order) co-occurrences over all noun phrases is determined.\n",
    "    <p/>\n",
    "    <p>\n",
    "        This distribution is compared with the overall distribution of co-occurrences over\n",
    "        noun phrases. The larger the difference between the two distributions (measured using\n",
    "        the Kullback-Leibler distance), the higher the relevance of a noun phrase. Intuitively,\n",
    "        the idea is that noun phrases with a low relevance (or noun phrases with a general\n",
    "        meaning), such as paper, interesting result, and new method, have a more or less\n",
    "        equal distribution of their (second-order) co-occurrences. On the other hand, noun\n",
    "        phrases with a high relevance (or noun phrases with a specific meaning), such as\n",
    "        visualization, text mining, and natural language processing, have a distribution of\n",
    "        their (second-order) co-occurrences that is significantly biased towards certain other\n",
    "        noun phrases. Hence, it is assumed that in a co-occurrence network noun phrases with\n",
    "        a high relevance are grouped together into clusters. Each cluster may be seen as a\n",
    "        topic.\n",
    "    </p>\n",
    "</blockquote>\n",
    "\n",
    "We  won't be using Apache OpenNLP to perform POS tagging like they did, because there was an issue in the later build that we tried, and it was inconvenient to use because it's a Java library. Instead, we will first be using nltk, which is a well liked Python package for NLP.\n",
    "\n",
    "Aside from nltk, we might consider those other promising NLP tools :\n",
    "- Google Parsey McParseface\n",
    "- Stanford CoreNLP\n",
    "- Amazon Comprehend\n",
    "- Flair\n",
    "- Gensim\n",
    "- Spacy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cdcd0b4-7f77-44b4-81a0-e8ee745c06f9",
   "metadata": {},
   "source": [
    "## Setup\n",
    "We first import the dependencies, and download the models used for pos tagging and tokenising."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5324dcb1-f243-4019-a623-0b40bf853f59",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/moonlyss/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to /home/moonlyss/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package tagsets to /home/moonlyss/nltk_data...\n",
      "[nltk_data]   Package tagsets is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/moonlyss/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.7.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /home/moonlyss/conda/envs/3.12/lib/python3.12/site-packages (from en-core-web-sm==3.7.1) (3.7.5)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /home/moonlyss/conda/envs/3.12/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/moonlyss/conda/envs/3.12/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/moonlyss/conda/envs/3.12/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/moonlyss/conda/envs/3.12/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/moonlyss/conda/envs/3.12/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /home/moonlyss/conda/envs/3.12/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.4)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /home/moonlyss/conda/envs/3.12/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /home/moonlyss/conda/envs/3.12/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/moonlyss/conda/envs/3.12/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /home/moonlyss/conda/envs/3.12/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /home/moonlyss/conda/envs/3.12/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.12.3)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/moonlyss/conda/envs/3.12/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/moonlyss/conda/envs/3.12/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /home/moonlyss/.local/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.6.3)\n",
      "Requirement already satisfied: jinja2 in /home/moonlyss/conda/envs/3.12/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.3)\n",
      "Requirement already satisfied: setuptools in /home/moonlyss/.local/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (69.1.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/moonlyss/conda/envs/3.12/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (23.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/moonlyss/conda/envs/3.12/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /home/moonlyss/conda/envs/3.12/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.4)\n",
      "Requirement already satisfied: language-data>=1.2 in /home/moonlyss/conda/envs/3.12/lib/python3.12/site-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /home/moonlyss/.local/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.3 in /home/moonlyss/.local/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.16.3)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /home/moonlyss/.local/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.10.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/moonlyss/conda/envs/3.12/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/moonlyss/.local/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/moonlyss/conda/envs/3.12/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/moonlyss/conda/envs/3.12/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.2.2)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /home/moonlyss/conda/envs/3.12/lib/python3.12/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /home/moonlyss/conda/envs/3.12/lib/python3.12/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.5)\n",
      "Requirement already satisfied: click>=8.0.0 in /home/moonlyss/.local/lib/python3.12/site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /home/moonlyss/conda/envs/3.12/lib/python3.12/site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /home/moonlyss/conda/envs/3.12/lib/python3.12/site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (13.7.1)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /home/moonlyss/conda/envs/3.12/lib/python3.12/site-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.18.1)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /home/moonlyss/conda/envs/3.12/lib/python3.12/site-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (7.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/moonlyss/conda/envs/3.12/lib/python3.12/site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.5)\n",
      "Requirement already satisfied: marisa-trie>=0.7.7 in /home/moonlyss/conda/envs/3.12/lib/python3.12/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/moonlyss/conda/envs/3.12/lib/python3.12/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/moonlyss/conda/envs/3.12/lib/python3.12/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.17.2)\n",
      "Requirement already satisfied: wrapt in /home/moonlyss/conda/envs/3.12/lib/python3.12/site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.16.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/moonlyss/conda/envs/3.12/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.2)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import polars as pl\n",
    "from pprint import pp\n",
    "import os\n",
    "import spacy\n",
    "\n",
    "adj_tags = [\"JJ\", \"JJR\", \"JJS\"]\n",
    "noun_tags = [\"NN\", \"NNS\", \"NNP\", \"NNPS\"]\n",
    "\n",
    "def df(name: str) -> str:\n",
    "    return os.path.join(\"dataframes\", name + \".parquet\")\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')  # pos tagger\n",
    "nltk.download('punkt')  # tokenizer\n",
    "nltk.download('tagsets')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689b8225-5fde-41c5-b8b5-a3c21d9fb652",
   "metadata": {},
   "source": [
    "##  Identification of noun phrases\n",
    "### Evaluating part-of-speech tagging (POS)\n",
    "Let's start with taking an abstract from our BIM papers dataset and applying POS tagging with nltk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e17ab462-6483-4def-bfb8-4a0e28ec2254",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successful tagging : \n",
      "[('Abstract', 'NN'),\n",
      " (':', ':'),\n",
      " ('The', 'DT'),\n",
      " ('development', 'NN'),\n",
      " ('of', 'IN'),\n",
      " ('the', 'DT'),\n",
      " ('digital', 'JJ'),\n",
      " ('economy', 'NN'),\n",
      " ('has', 'VBZ'),\n",
      " ('changed', 'VBN')]\n",
      "\n",
      "Tagging failure, 'stricter' should be an adjective (JJ) :\n",
      "[('cities', 'NNS'),\n",
      " ('with', 'IN'),\n",
      " ('stricter', 'NN'),\n",
      " ('governmental', 'JJ'),\n",
      " ('environmental', 'JJ'),\n",
      " ('regulations', 'NNS')]\n"
     ]
    }
   ],
   "source": [
    "abstract = pl.scan_parquet(df(\"papers\")).first().collect()[0, \"Abstract\"]\n",
    "tokens = nltk.word_tokenize(abstract)\n",
    "tags = nltk.pos_tag(tokens)\n",
    "print(\"Successful tagging : \")\n",
    "pp(tags[:10])\n",
    "print(\"\\nTagging failure, 'stricter' should be an adjective (JJ) :\")\n",
    "pp(tags[219:225])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c57b3ee-51fc-416d-921e-2b532c57dfb1",
   "metadata": {},
   "source": [
    "The result seems convincing but not perfect. We can move on to the filtering and extraction of noun phrases.\n",
    "\n",
    "Method used by VOSviewer :\n",
    "<blockquote>\n",
    "We then use a linguistic filter to identify noun phrases. Our filter selects all\n",
    "word sequences that consist exclusively of nouns and adjectives and that end with a\n",
    "noun (e.g., paper, visualization, interesting result, and text mining, but not degrees of\n",
    "freedom and highly cited publication). Finally, we convert plural noun phrases into\n",
    "singular ones.  \n",
    "</blockquote>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "121d7991-103e-4769-90d2-250afc9e323a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (75, 2)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>noun_phrase</th><th>tags</th></tr><tr><td>str</td><td>list[cat]</td></tr></thead><tbody><tr><td>&quot;fixed effect model&quot;</td><td>[&quot;JJ&quot;, &quot;NN&quot;, &quot;NN&quot;]</td></tr><tr><td>&quot;other robustness tests&quot;</td><td>[&quot;JJ&quot;, &quot;NN&quot;, &quot;NNS&quot;]</td></tr><tr><td>&quot;study&quot;</td><td>[&quot;NN&quot;]</td></tr><tr><td>&quot;urban level&quot;</td><td>[&quot;JJ&quot;, &quot;NN&quot;]</td></tr><tr><td>&quot;super-efficient slack-based me…</td><td>[&quot;JJ&quot;, &quot;JJ&quot;, &quot;NN&quot;]</td></tr><tr><td>&hellip;</td><td>&hellip;</td></tr><tr><td>&quot;cities&quot;</td><td>[&quot;NNS&quot;]</td></tr><tr><td>&quot;sdgs&quot;</td><td>[&quot;NNP&quot;]</td></tr><tr><td>&quot;mechanism analysis&quot;</td><td>[&quot;NN&quot;, &quot;NN&quot;]</td></tr><tr><td>&quot;urban gee&quot;</td><td>[&quot;JJ&quot;, &quot;NNP&quot;]</td></tr><tr><td>&quot;energy structure transformatio…</td><td>[&quot;NN&quot;, &quot;NN&quot;, &quot;NN&quot;]</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (75, 2)\n",
       "┌─────────────────────────────────┬─────────────────────┐\n",
       "│ noun_phrase                     ┆ tags                │\n",
       "│ ---                             ┆ ---                 │\n",
       "│ str                             ┆ list[cat]           │\n",
       "╞═════════════════════════════════╪═════════════════════╡\n",
       "│ fixed effect model              ┆ [\"JJ\", \"NN\", \"NN\"]  │\n",
       "│ other robustness tests          ┆ [\"JJ\", \"NN\", \"NNS\"] │\n",
       "│ study                           ┆ [\"NN\"]              │\n",
       "│ urban level                     ┆ [\"JJ\", \"NN\"]        │\n",
       "│ super-efficient slack-based me… ┆ [\"JJ\", \"JJ\", \"NN\"]  │\n",
       "│ …                               ┆ …                   │\n",
       "│ cities                          ┆ [\"NNS\"]             │\n",
       "│ sdgs                            ┆ [\"NNP\"]             │\n",
       "│ mechanism analysis              ┆ [\"NN\", \"NN\"]        │\n",
       "│ urban gee                       ┆ [\"JJ\", \"NNP\"]       │\n",
       "│ energy structure transformatio… ┆ [\"NN\", \"NN\", \"NN\"]  │\n",
       "└─────────────────────────────────┴─────────────────────┘"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "           \n",
    "filtered_q = (\n",
    "    pl.DataFrame(tags, orient=\"row\", schema={\"token\": pl.String, \"tag\": pl.Categorical(ordering=\"physical\")}).lazy()\n",
    "    .select(\n",
    "        pl.arange(0, pl.len()).alias(\"pos\"),\n",
    "        pl.all(),\n",
    "        pl.col(\"tag\").is_in(adj_tags + noun_tags).alias(\"keep\")  # keep only nouns and adjectives\n",
    "    )                  \n",
    "    .with_columns(group_id=pl.col(\"keep\").rle_id())\n",
    "    .filter(pl.col(\"keep\") == True)\n",
    "    .select(\n",
    "        \"pos\", \n",
    "        \"token\",\n",
    "        \"tag\",\n",
    "        \"group_id\")\n",
    "    .group_by(\"group_id\").agg(\"pos\", \"token\", \"tag\")\n",
    "    \n",
    "    # Keep only word sequences that end with a noun\n",
    "    .with_columns(\n",
    "        pl.col(\"tag\").list.eval(pl.element().is_in(adj_tags)).list.reverse().alias(\"adjs_reversed\")\n",
    "    )\n",
    "    .with_columns(\n",
    "        (pl.col(\"adjs_reversed\").list.len() - 1 - pl.col(\"adjs_reversed\").list.arg_min()).alias(\"last_noun_pos\"),\n",
    "    )\n",
    "    .with_columns(\n",
    "        pl.col(\"token\").list.head(pl.col(\"last_noun_pos\") + 1)\n",
    "    )\n",
    "    \n",
    "    .filter(~pl.col(\"adjs_reversed\").list.all())  # remove groups that are only adjectives\n",
    "    .select(\n",
    "        pl.col(\"token\").list.join(\" \").str.to_lowercase().alias(\"noun_phrase\"),\n",
    "        pl.col(\"tag\").alias(\"tags\"),\n",
    "    )\n",
    ")\n",
    "\n",
    "with pl.StringCache():\n",
    "    pl.Series(\"tags_order\", [\"NN\", \"NNS\", \"JJ\"], pl.Categorical)\n",
    "    filtered = filtered_q.collect()\n",
    "filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a797440a-3991-4a9d-b32f-f8911ca30e81",
   "metadata": {},
   "source": [
    "Now that we have a working solution for one text, let's adapt it to make it work on the complete dataframe of papers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "693c59b0-2684-4317-b1a0-61aefcebbe6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\", disable = ['ner'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "835ac3e3-ef65-4297-a317-591c5e335f82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My my PRON PRP$ poss Xx True True\n",
      "beautiful beautiful ADJ JJ amod xxxx True False\n",
      "dogs dog NOUN NNS nsubj xxxx True False\n",
      "are be AUX VBP ROOT xxx True True\n",
      "better well ADJ JJR acomp xxxx True False\n",
      "than than ADP IN prep xxxx True True\n",
      "yours your NOUN NNS pobj xxxx True True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "doc = nlp(\"My beautiful dogs are better than yours\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
    "            token.shape_, token.is_alpha, token.is_stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac523ab-910d-4b5d-8865-f75dd98fa89d",
   "metadata": {},
   "source": [
    "We now create a dataframe where we apply the strategy of VOSviewer to all papers in our dataset.\n",
    "Resulting dataframes has the following columns :\n",
    "- noun_phrase\n",
    "- paper_ids : multiple noun phrase appearances in paper will result in several ids appearances\n",
    "- tags\n",
    "- count\n",
    "- paper_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "id": "e8a68fa6-b0a9-4bdb-8e99-02574db8e63c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.36829662322998\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (872, 6)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>id</th><th>term</th><th>paper_ids</th><th>count</th><th>tags</th><th>paper_count</th></tr><tr><td>i64</td><td>str</td><td>list[i64]</td><td>u32</td><td>list[cat]</td><td>u32</td></tr></thead><tbody><tr><td>0</td><td>&quot;technology&quot;</td><td>[232, 479, … 73]</td><td>231</td><td>[&quot;NNS&quot;]</td><td>159</td></tr><tr><td>1</td><td>&quot;sustainability&quot;</td><td>[205, 475, … 368]</td><td>199</td><td>[&quot;NN&quot;]</td><td>142</td></tr><tr><td>2</td><td>&quot;result&quot;</td><td>[308, 63, … 199]</td><td>152</td><td>[&quot;NNS&quot;]</td><td>125</td></tr><tr><td>3</td><td>&quot;development&quot;</td><td>[53, 184, … 326]</td><td>143</td><td>[&quot;NN&quot;]</td><td>117</td></tr><tr><td>4</td><td>&quot;research&quot;</td><td>[182, 362, … 420]</td><td>150</td><td>[&quot;NN&quot;]</td><td>112</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>867</td><td>&quot;ict development&quot;</td><td>[238, 238, … 238]</td><td>6</td><td>[&quot;NN&quot;, &quot;NN&quot;]</td><td>1</td></tr><tr><td>868</td><td>&quot;firm green innovation&quot;</td><td>[49, 49, … 49]</td><td>6</td><td>[&quot;NN&quot;, &quot;JJ&quot;, &quot;NN&quot;]</td><td>1</td></tr><tr><td>869</td><td>&quot;digital placemaking&quot;</td><td>[108, 108, … 108]</td><td>6</td><td>[&quot;JJ&quot;, &quot;NN&quot;]</td><td>1</td></tr><tr><td>870</td><td>&quot;social medium usage&quot;</td><td>[231, 231, … 231]</td><td>6</td><td>[&quot;JJ&quot;, &quot;NNS&quot;, &quot;NN&quot;]</td><td>1</td></tr><tr><td>871</td><td>&quot;dm&quot;</td><td>[216, 216, … 216]</td><td>6</td><td>[&quot;NN&quot;]</td><td>1</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (872, 6)\n",
       "┌─────┬───────────────────────┬───────────────────┬───────┬─────────────────────┬─────────────┐\n",
       "│ id  ┆ term                  ┆ paper_ids         ┆ count ┆ tags                ┆ paper_count │\n",
       "│ --- ┆ ---                   ┆ ---               ┆ ---   ┆ ---                 ┆ ---         │\n",
       "│ i64 ┆ str                   ┆ list[i64]         ┆ u32   ┆ list[cat]           ┆ u32         │\n",
       "╞═════╪═══════════════════════╪═══════════════════╪═══════╪═════════════════════╪═════════════╡\n",
       "│ 0   ┆ technology            ┆ [232, 479, … 73]  ┆ 231   ┆ [\"NNS\"]             ┆ 159         │\n",
       "│ 1   ┆ sustainability        ┆ [205, 475, … 368] ┆ 199   ┆ [\"NN\"]              ┆ 142         │\n",
       "│ 2   ┆ result                ┆ [308, 63, … 199]  ┆ 152   ┆ [\"NNS\"]             ┆ 125         │\n",
       "│ 3   ┆ development           ┆ [53, 184, … 326]  ┆ 143   ┆ [\"NN\"]              ┆ 117         │\n",
       "│ 4   ┆ research              ┆ [182, 362, … 420] ┆ 150   ┆ [\"NN\"]              ┆ 112         │\n",
       "│ …   ┆ …                     ┆ …                 ┆ …     ┆ …                   ┆ …           │\n",
       "│ 867 ┆ ict development       ┆ [238, 238, … 238] ┆ 6     ┆ [\"NN\", \"NN\"]        ┆ 1           │\n",
       "│ 868 ┆ firm green innovation ┆ [49, 49, … 49]    ┆ 6     ┆ [\"NN\", \"JJ\", \"NN\"]  ┆ 1           │\n",
       "│ 869 ┆ digital placemaking   ┆ [108, 108, … 108] ┆ 6     ┆ [\"JJ\", \"NN\"]        ┆ 1           │\n",
       "│ 870 ┆ social medium usage   ┆ [231, 231, … 231] ┆ 6     ┆ [\"JJ\", \"NNS\", \"NN\"] ┆ 1           │\n",
       "│ 871 ┆ dm                    ┆ [216, 216, … 216] ┆ 6     ┆ [\"NN\"]              ┆ 1           │\n",
       "└─────┴───────────────────────┴───────────────────┴───────┴─────────────────────┴─────────────┘"
      ]
     },
     "execution_count": 387,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# from flair.nn import Classifier\n",
    "# from flair.data import Sentence\n",
    "\n",
    "def nltk_tags(col: str, alias: str | None=None):\n",
    "    return (pl.col(col).alias(alias if alias is not None else col.lower() + \"_tokens_tags\")\n",
    "        .map_elements(nltk.word_tokenize, return_dtype=pl.List(pl.String), strategy=\"thread_local\")\n",
    "        .map_batches(lambda series : pl.Series(nltk.pos_tag_sents(series)), return_dtype=pl.List(pl.List(pl.String)))\n",
    "            # .cast(pl.Struct({\"yo\": pl.String, \"y\": pl.Categorical}))\n",
    "           )\n",
    "from nltk.stem import WordNetLemmatizer\n",
    " \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "# # load the model\n",
    "# tagger = Classifier.load('pos-fast')\n",
    "\n",
    "# # make a sentence\n",
    "# sentence = Sentence('Dirk went to the store.')\n",
    "\n",
    "# # predict NER tags\n",
    "skip_words = [\"abstract\", \"%\",\n",
    "              \"paper\", \"study\", \"use\"\n",
    "             ]\n",
    "t0 = time.time()\n",
    "# tagger.predict(sentence)\n",
    "\n",
    "# print sentence with predicted tags\n",
    "q = (\n",
    "    pl.scan_parquet(df(\"papers\"))\n",
    "    .select(\n",
    "        pl.col(\"Id\").alias(\"paper_id\"), \n",
    "        pl.concat_list(\n",
    "            pl.col(\"Subject\"),\n",
    "            pl.col(\"Abstract\"),\n",
    "            pl.col(\"Keywords\")\n",
    "        ).alias(\"text\"),   \n",
    "    )\n",
    "    .explode(\"text\")\n",
    "    .with_columns(\n",
    "        pl.arange(0, pl.len()).alias(\"text_id\"),\n",
    "        nltk_tags(\"text\", \"tokens_tags\"),\n",
    "    )\n",
    "    .explode(\"tokens_tags\")\n",
    "    .select(\n",
    "        pl.exclude(\"tokens_tags\"),\n",
    "        pl.col(\"tokens_tags\").list.get(0).alias(\"token\"),\n",
    "        pl.col(\"tokens_tags\").list.get(1).alias(\"tag\").cast(pl.Categorical)\n",
    "    ) \n",
    "    .with_columns(\n",
    "        pl.col(\"tag\").is_in(adj_tags + noun_tags).alias(\"keep\")  # keep only nouns and adjectives\n",
    "    )                  \n",
    "    .with_columns(group_id=pl.col(\"keep\").rle_id())\n",
    "    .filter(pl.col(\"keep\") == True)\n",
    "    \n",
    "    # Lemmatize plural forms to singular form\n",
    "    .with_columns(\n",
    "        pl.struct(\"token\", \"tag\").map_elements(lambda t: lemmatizer.lemmatize(t[\"token\"]) if t[\"tag\"] in noun_tags else t[\"token\"], return_dtype=pl.String).alias(\"token\"),\n",
    "    )\n",
    "    .group_by(\"paper_id\", \"text_id\", \"group_id\").agg(\"token\", \"tag\")\n",
    "    \n",
    "    # Keep only word sequences that end with a noun\n",
    "    .with_columns(\n",
    "        pl.col(\"tag\").list.eval(pl.element().is_in(adj_tags)).list.reverse().alias(\"adjs_reversed\")\n",
    "    )\n",
    "    .with_columns(\n",
    "        (pl.col(\"adjs_reversed\").list.len() - 1 - pl.col(\"adjs_reversed\").list.arg_min()).alias(\"last_noun_pos\"),\n",
    "    )\n",
    "    .with_columns(\n",
    "        pl.col(\"token\").list.head(pl.col(\"last_noun_pos\") + 1)\n",
    "    )\n",
    "    .filter(~pl.col(\"adjs_reversed\").list.all())  # remove groups that are only adjectives\n",
    "\n",
    "    # Final data preparation\n",
    "    .select(\n",
    "        pl.col(\"paper_id\").alias(\"paper_ids\"),\n",
    "        pl.col(\"token\").list.join(\" \").str.to_lowercase().alias(\"term\"),\n",
    "        pl.col(\"tag\").alias(\"tags\"),\n",
    "    )\n",
    "    .filter(~pl.col(\"term\").is_in(skip_words), )\n",
    "    .group_by(\"term\").agg(\n",
    "        pl.col(\"paper_ids\"), pl.col(\"term\").len().alias(\"count\"),\n",
    "        pl.col(\"tags\").first()\n",
    "    )\n",
    "    .filter(pl.col(\"count\") > 5)  # keep only words that appear 5 times\n",
    "    .with_columns(pl.col(\"paper_ids\").list.unique().list.len().alias(\"paper_count\"))\n",
    "    \n",
    "    .sort(\"paper_count\", \"count\", descending=True)\n",
    "    .select(\n",
    "        pl.arange(0, pl.len()).alias(\"id\"),\n",
    "        pl.all()\n",
    "    )\n",
    ")\n",
    "\n",
    "res = q.collect()\n",
    "# res = q.fetch(50)\n",
    "print(time.time() - t0)\n",
    "res.write_parquet(df(\"terms_from_subject_abstract_keywords\"))\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "id": "e7880717-a67b-4e83-9a40-787df6070367",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (11_582, 4)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>id</th><th>term</th><th>paper_id</th><th>count</th></tr><tr><td>i64</td><td>cat</td><td>i64</td><td>u32</td></tr></thead><tbody><tr><td>41</td><td>&quot;environment&quot;</td><td>365</td><td>1</td></tr><tr><td>221</td><td>&quot;smart technology&quot;</td><td>206</td><td>1</td></tr><tr><td>291</td><td>&quot;information modeling&quot;</td><td>267</td><td>1</td></tr><tr><td>67</td><td>&quot;adoption&quot;</td><td>441</td><td>1</td></tr><tr><td>102</td><td>&quot;mobility&quot;</td><td>188</td><td>1</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>376</td><td>&quot;transparency&quot;</td><td>15</td><td>1</td></tr><tr><td>299</td><td>&quot;b&quot;</td><td>148</td><td>1</td></tr><tr><td>163</td><td>&quot;mechanism&quot;</td><td>179</td><td>1</td></tr><tr><td>207</td><td>&quot;concern&quot;</td><td>484</td><td>1</td></tr><tr><td>382</td><td>&quot;topic&quot;</td><td>213</td><td>1</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (11_582, 4)\n",
       "┌─────┬──────────────────────┬──────────┬───────┐\n",
       "│ id  ┆ term                 ┆ paper_id ┆ count │\n",
       "│ --- ┆ ---                  ┆ ---      ┆ ---   │\n",
       "│ i64 ┆ cat                  ┆ i64      ┆ u32   │\n",
       "╞═════╪══════════════════════╪══════════╪═══════╡\n",
       "│ 41  ┆ environment          ┆ 365      ┆ 1     │\n",
       "│ 221 ┆ smart technology     ┆ 206      ┆ 1     │\n",
       "│ 291 ┆ information modeling ┆ 267      ┆ 1     │\n",
       "│ 67  ┆ adoption             ┆ 441      ┆ 1     │\n",
       "│ 102 ┆ mobility             ┆ 188      ┆ 1     │\n",
       "│ …   ┆ …                    ┆ …        ┆ …     │\n",
       "│ 376 ┆ transparency         ┆ 15       ┆ 1     │\n",
       "│ 299 ┆ b                    ┆ 148      ┆ 1     │\n",
       "│ 163 ┆ mechanism            ┆ 179      ┆ 1     │\n",
       "│ 207 ┆ concern              ┆ 484      ┆ 1     │\n",
       "│ 382 ┆ topic                ┆ 213      ┆ 1     │\n",
       "└─────┴──────────────────────┴──────────┴───────┘"
      ]
     },
     "execution_count": 390,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from itertools import combinationsimport jax.numpy as jnp\n",
    "import jax.experimental.sparse as sparse\n",
    "\n",
    "res = (\n",
    "    pl.scan_parquet(df(\"terms_from_subject_abstract_keywords\"))\n",
    "    .explode(\"paper_ids\")\n",
    "    .with_columns(pl.col(\"term\").cast(pl.Categorical))\n",
    "    .group_by(\"id\", \"term\", \"paper_ids\").agg(pl.col(\"term\").len().alias(\"count\"),\n",
    "                                       # pl.col(\"tags\").first()\n",
    "                                      )\n",
    "    .rename({\"paper_ids\": \"paper_id\"})\n",
    "    # .group_by(\"paper_id\").agg(pl.struct(term=pl.col(\"term\"), count=pl.col(\"count\")).alias(\"terms\"))\n",
    "    # .filter(pl.col(\"paper_id\") == 265)\n",
    "    # .with_columns(\n",
    "    #     pl.col(\"terms\")\n",
    "    #     # .map_elements(\n",
    "    #     #     lambda a: print(a),\n",
    "    #     #     # return_dtype=pl.List(pl.List(pl.Struct({\"term\":pl.Categorical, \"count\":pl.UInt32})))\n",
    "    #     # )\n",
    "    #     ,\n",
    "    #     pl.col(\"terms\").list.len().alias(\"term_count\")\n",
    "    # )\n",
    "    # .select(pl.exclude(\"term_count\"))\n",
    "    # .explode(\"terms\")\n",
    "    # .unnest(\"terms\")\n",
    ").collect()\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "id": "a357f93b-cb3a-4409-9485-b9a24a7c2ce0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BCOO(uint32[872, 543], nse=11582)"
      ]
     },
     "execution_count": 420,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = res[\"count\"].to_numpy()\n",
    "indices = np.concatenate(res.select(pl.concat_list(\"id\", \"paper_id\").alias(\"indices\"))[\"indices\"].to_numpy()).reshape(len(res), 2)\n",
    "kw_paper = sparse.BCOO((data, indices), shape=(872, 543))\n",
    "kw_paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "id": "532007b9-7390-478c-ba77-3bc0ada8f4c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (174, 4)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>id</th><th>term</th><th>paper_id</th><th>count</th></tr><tr><td>i64</td><td>cat</td><td>i64</td><td>u32</td></tr></thead><tbody><tr><td>6</td><td>&quot;digital technology&quot;</td><td>207</td><td>6</td></tr><tr><td>821</td><td>&quot;urban underground space resour…</td><td>476</td><td>8</td></tr><tr><td>215</td><td>&quot;education&quot;</td><td>384</td><td>7</td></tr><tr><td>826</td><td>&quot;hbmm&quot;</td><td>350</td><td>8</td></tr><tr><td>84</td><td>&quot;bim&quot;</td><td>262</td><td>9</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>70</td><td>&quot;chapter&quot;</td><td>140</td><td>8</td></tr><tr><td>317</td><td>&quot;enterprise&quot;</td><td>179</td><td>7</td></tr><tr><td>538</td><td>&quot;scheme&quot;</td><td>208</td><td>6</td></tr><tr><td>859</td><td>&quot;sponge city strategy&quot;</td><td>440</td><td>6</td></tr><tr><td>12</td><td>&quot;industry&quot;</td><td>25</td><td>7</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (174, 4)\n",
       "┌─────┬─────────────────────────────────┬──────────┬───────┐\n",
       "│ id  ┆ term                            ┆ paper_id ┆ count │\n",
       "│ --- ┆ ---                             ┆ ---      ┆ ---   │\n",
       "│ i64 ┆ cat                             ┆ i64      ┆ u32   │\n",
       "╞═════╪═════════════════════════════════╪══════════╪═══════╡\n",
       "│ 6   ┆ digital technology              ┆ 207      ┆ 6     │\n",
       "│ 821 ┆ urban underground space resour… ┆ 476      ┆ 8     │\n",
       "│ 215 ┆ education                       ┆ 384      ┆ 7     │\n",
       "│ 826 ┆ hbmm                            ┆ 350      ┆ 8     │\n",
       "│ 84  ┆ bim                             ┆ 262      ┆ 9     │\n",
       "│ …   ┆ …                               ┆ …        ┆ …     │\n",
       "│ 70  ┆ chapter                         ┆ 140      ┆ 8     │\n",
       "│ 317 ┆ enterprise                      ┆ 179      ┆ 7     │\n",
       "│ 538 ┆ scheme                          ┆ 208      ┆ 6     │\n",
       "│ 859 ┆ sponge city strategy            ┆ 440      ┆ 6     │\n",
       "│ 12  ┆ industry                        ┆ 25       ┆ 7     │\n",
       "└─────┴─────────────────────────────────┴──────────┴───────┘"
      ]
     },
     "execution_count": 423,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.filter(pl.col(\"count\") > 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "id": "0022be79-1415-4ca3-9979-140fd52a74e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax.experimental.sparse import sparsify\n",
    "import jax\n",
    "dot_sp = sparsify(jnp.dot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6051b848-574a-4b40-be63-a610ae015966",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def get_cooccurences(a):\n",
    "    return dot_sp(a, a.T)\n",
    "\n",
    "cooccurences = dot_sp(kw_paper, kw_paper.T)\n",
    "cooccurences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "id": "044d8247-2261-417a-ba24-b8d0e09a5ee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-06 10:41:07.855695: W external/tsl/tsl/framework/bfc_allocator.cc:482] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.62GiB (rounded to 2817208320)requested by op \n",
      "2024-06-06 10:41:07.855889: W external/tsl/tsl/framework/bfc_allocator.cc:494] *********************************************************************************************____***\n",
      "E0606 10:41:07.856171  108300 pjrt_stream_executor_client.cc:2826] Execution of replica 0 failed: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 2817208104 bytes.\n",
      "BufferAssignment OOM Debugging.\n",
      "BufferAssignment stats:\n",
      "             parameter allocation:   135.7KiB\n",
      "              constant allocation:         8B\n",
      "        maybe_live_out allocation:    8.70MiB\n",
      "     preallocated temp allocation:    2.62GiB\n",
      "                 total allocation:    2.63GiB\n",
      "Peak buffers:\n",
      "\tBuffer 1:\n",
      "\t\tSize: 511.71MiB\n",
      "\t\tOperator: op_name=\"jit(get_cooccurences)/jit(main)/jit(_unique_sorted_mask)/jit(lexsort)/sort[dimension=0 is_stable=True num_keys=2]\" source_file=\"/tmp/ipykernel_108300/2930833792.py\" source_line=3\n",
      "\t\tXLA Label: sort\n",
      "\t\tShape: s32[134142724]\n",
      "\t\t==========================\n",
      "\n",
      "\tBuffer 2:\n",
      "\t\tSize: 511.71MiB\n",
      "\t\tOperator: op_name=\"jit(get_cooccurences)/jit(main)/jit(_unique_sorted_mask)/jit(lexsort)/sort[dimension=0 is_stable=True num_keys=2]\" source_file=\"/tmp/ipykernel_108300/2930833792.py\" source_line=3\n",
      "\t\tXLA Label: sort\n",
      "\t\tShape: s32[134142724]\n",
      "\t\t==========================\n",
      "\n",
      "\tBuffer 3:\n",
      "\t\tSize: 511.71MiB\n",
      "\t\tOperator: op_name=\"jit(get_cooccurences)/jit(main)/jit(_unique_sorted_mask)/jit(lexsort)/iota[dtype=int32 shape=(134142724,) dimension=0]\" source_file=\"/tmp/ipykernel_108300/2930833792.py\" source_line=3\n",
      "\t\tXLA Label: fusion\n",
      "\t\tShape: s32[134142724]\n",
      "\t\t==========================\n",
      "\n",
      "\tBuffer 4:\n",
      "\t\tSize: 511.71MiB\n",
      "\t\tOperator: op_name=\"jit(get_cooccurences)/jit(main)/jit(_unique_sorted_mask)/jit(lexsort)/slice[start_indices=(1, 0) limit_indices=(2, 134142724) strides=(1, 1)]\" source_file=\"/tmp/ipykernel_108300/2930833792.py\" source_line=3\n",
      "\t\tXLA Label: fusion\n",
      "\t\tShape: s32[1,134142724]\n",
      "\t\t==========================\n",
      "\n",
      "\tBuffer 5:\n",
      "\t\tSize: 511.71MiB\n",
      "\t\tOperator: op_name=\"jit(get_cooccurences)/jit(main)/jit(_unique_sorted_mask)/jit(lexsort)/slice[start_indices=(1, 0) limit_indices=(2, 134142724) strides=(1, 1)]\" source_file=\"/tmp/ipykernel_108300/2930833792.py\" source_line=3\n",
      "\t\tXLA Label: fusion\n",
      "\t\tShape: s32[1,134142724]\n",
      "\t\t==========================\n",
      "\n",
      "\tBuffer 6:\n",
      "\t\tSize: 127.93MiB\n",
      "\t\tOperator: op_name=\"jit(get_cooccurences)/jit(main)/reduce_or[axes=(1,)]\" source_file=\"/tmp/ipykernel_108300/4272418259.py\" source_line=3\n",
      "\t\tXLA Label: fusion\n",
      "\t\tShape: pred[11582,11582]\n",
      "\t\t==========================\n",
      "\n",
      "\tBuffer 7:\n",
      "\t\tSize: 5.80MiB\n",
      "\t\tOperator: op_name=\"jit(get_cooccurences)/jit(main)/gather[dimension_numbers=GatherDimensionNumbers(offset_dims=(0, 1), collapsed_slice_dims=(), start_index_map=(0,)) slice_sizes=(760384, 2) unique_indices=True indices_are_sorted=True mode=GatherScatterMode.PROMISE_IN_BOUNDS fill_value=None]\" source_file=\"/tmp/ipykernel_108300/4272418259.py\" source_line=3\n",
      "\t\tXLA Label: fusion\n",
      "\t\tShape: s32[1,760384,2]\n",
      "\t\t==========================\n",
      "\n",
      "\tBuffer 8:\n",
      "\t\tSize: 2.90MiB\n",
      "\t\tXLA Label: fusion\n",
      "\t\tShape: u32[760384]\n",
      "\t\t==========================\n",
      "\n",
      "\tBuffer 9:\n",
      "\t\tSize: 90.5KiB\n",
      "\t\tEntry Parameter Subshape: s32[11582,2]\n",
      "\t\t==========================\n",
      "\n",
      "\tBuffer 10:\n",
      "\t\tSize: 45.2KiB\n",
      "\t\tXLA Label: fusion\n",
      "\t\tShape: s32[11582,1]\n",
      "\t\t==========================\n",
      "\n",
      "\tBuffer 11:\n",
      "\t\tSize: 45.2KiB\n",
      "\t\tOperator: op_name=\"jit(get_cooccurences)/jit(main)/lt\" source_file=\"/tmp/ipykernel_108300/4272418259.py\" source_line=3\n",
      "\t\tXLA Label: fusion\n",
      "\t\tShape: s32[1,11582]\n",
      "\t\t==========================\n",
      "\n",
      "\tBuffer 12:\n",
      "\t\tSize: 45.2KiB\n",
      "\t\tOperator: op_name=\"jit(get_cooccurences)/jit(main)/lt\" source_file=\"/tmp/ipykernel_108300/4272418259.py\" source_line=3\n",
      "\t\tXLA Label: fusion\n",
      "\t\tShape: s32[1,1,11582]\n",
      "\t\t==========================\n",
      "\n",
      "\tBuffer 13:\n",
      "\t\tSize: 45.2KiB\n",
      "\t\tOperator: op_name=\"jit(get_cooccurences)/jit(main)/lt\" source_file=\"/tmp/ipykernel_108300/4272418259.py\" source_line=3\n",
      "\t\tXLA Label: fusion\n",
      "\t\tShape: s32[11582,1]\n",
      "\t\t==========================\n",
      "\n",
      "\tBuffer 14:\n",
      "\t\tSize: 45.2KiB\n",
      "\t\tEntry Parameter Subshape: u32[11582]\n",
      "\t\t==========================\n",
      "\n",
      "\tBuffer 15:\n",
      "\t\tSize: 11.3KiB\n",
      "\t\tOperator: op_name=\"jit(get_cooccurences)/jit(main)/lt\" source_file=\"/tmp/ipykernel_108300/4272418259.py\" source_line=3\n",
      "\t\tXLA Label: fusion\n",
      "\t\tShape: pred[1,1,11582]\n",
      "\t\t==========================\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "XlaRuntimeError",
     "evalue": "RESOURCE_EXHAUSTED: Out of memory while trying to allocate 2817208104 bytes.\nBufferAssignment OOM Debugging.\nBufferAssignment stats:\n             parameter allocation:   135.7KiB\n              constant allocation:         8B\n        maybe_live_out allocation:    8.70MiB\n     preallocated temp allocation:    2.62GiB\n                 total allocation:    2.63GiB\nPeak buffers:\n\tBuffer 1:\n\t\tSize: 511.71MiB\n\t\tOperator: op_name=\"jit(get_cooccurences)/jit(main)/jit(_unique_sorted_mask)/jit(lexsort)/sort[dimension=0 is_stable=True num_keys=2]\" source_file=\"/tmp/ipykernel_108300/2930833792.py\" source_line=3\n\t\tXLA Label: sort\n\t\tShape: s32[134142724]\n\t\t==========================\n\n\tBuffer 2:\n\t\tSize: 511.71MiB\n\t\tOperator: op_name=\"jit(get_cooccurences)/jit(main)/jit(_unique_sorted_mask)/jit(lexsort)/sort[dimension=0 is_stable=True num_keys=2]\" source_file=\"/tmp/ipykernel_108300/2930833792.py\" source_line=3\n\t\tXLA Label: sort\n\t\tShape: s32[134142724]\n\t\t==========================\n\n\tBuffer 3:\n\t\tSize: 511.71MiB\n\t\tOperator: op_name=\"jit(get_cooccurences)/jit(main)/jit(_unique_sorted_mask)/jit(lexsort)/iota[dtype=int32 shape=(134142724,) dimension=0]\" source_file=\"/tmp/ipykernel_108300/2930833792.py\" source_line=3\n\t\tXLA Label: fusion\n\t\tShape: s32[134142724]\n\t\t==========================\n\n\tBuffer 4:\n\t\tSize: 511.71MiB\n\t\tOperator: op_name=\"jit(get_cooccurences)/jit(main)/jit(_unique_sorted_mask)/jit(lexsort)/slice[start_indices=(1, 0) limit_indices=(2, 134142724) strides=(1, 1)]\" source_file=\"/tmp/ipykernel_108300/2930833792.py\" source_line=3\n\t\tXLA Label: fusion\n\t\tShape: s32[1,134142724]\n\t\t==========================\n\n\tBuffer 5:\n\t\tSize: 511.71MiB\n\t\tOperator: op_name=\"jit(get_cooccurences)/jit(main)/jit(_unique_sorted_mask)/jit(lexsort)/slice[start_indices=(1, 0) limit_indices=(2, 134142724) strides=(1, 1)]\" source_file=\"/tmp/ipykernel_108300/2930833792.py\" source_line=3\n\t\tXLA Label: fusion\n\t\tShape: s32[1,134142724]\n\t\t==========================\n\n\tBuffer 6:\n\t\tSize: 127.93MiB\n\t\tOperator: op_name=\"jit(get_cooccurences)/jit(main)/reduce_or[axes=(1,)]\" source_file=\"/tmp/ipykernel_108300/4272418259.py\" source_line=3\n\t\tXLA Label: fusion\n\t\tShape: pred[11582,11582]\n\t\t==========================\n\n\tBuffer 7:\n\t\tSize: 5.80MiB\n\t\tOperator: op_name=\"jit(get_cooccurences)/jit(main)/gather[dimension_numbers=GatherDimensionNumbers(offset_dims=(0, 1), collapsed_slice_dims=(), start_index_map=(0,)) slice_sizes=(760384, 2) unique_indices=True indices_are_sorted=True mode=GatherScatterMode.PROMISE_IN_BOUNDS fill_value=None]\" source_file=\"/tmp/ipykernel_108300/4272418259.py\" source_line=3\n\t\tXLA Label: fusion\n\t\tShape: s32[1,760384,2]\n\t\t==========================\n\n\tBuffer 8:\n\t\tSize: 2.90MiB\n\t\tXLA Label: fusion\n\t\tShape: u32[760384]\n\t\t==========================\n\n\tBuffer 9:\n\t\tSize: 90.5KiB\n\t\tEntry Parameter Subshape: s32[11582,2]\n\t\t==========================\n\n\tBuffer 10:\n\t\tSize: 45.2KiB\n\t\tXLA Label: fusion\n\t\tShape: s32[11582,1]\n\t\t==========================\n\n\tBuffer 11:\n\t\tSize: 45.2KiB\n\t\tOperator: op_name=\"jit(get_cooccurences)/jit(main)/lt\" source_file=\"/tmp/ipykernel_108300/4272418259.py\" source_line=3\n\t\tXLA Label: fusion\n\t\tShape: s32[1,11582]\n\t\t==========================\n\n\tBuffer 12:\n\t\tSize: 45.2KiB\n\t\tOperator: op_name=\"jit(get_cooccurences)/jit(main)/lt\" source_file=\"/tmp/ipykernel_108300/4272418259.py\" source_line=3\n\t\tXLA Label: fusion\n\t\tShape: s32[1,1,11582]\n\t\t==========================\n\n\tBuffer 13:\n\t\tSize: 45.2KiB\n\t\tOperator: op_name=\"jit(get_cooccurences)/jit(main)/lt\" source_file=\"/tmp/ipykernel_108300/4272418259.py\" source_line=3\n\t\tXLA Label: fusion\n\t\tShape: s32[11582,1]\n\t\t==========================\n\n\tBuffer 14:\n\t\tSize: 45.2KiB\n\t\tEntry Parameter Subshape: u32[11582]\n\t\t==========================\n\n\tBuffer 15:\n\t\tSize: 11.3KiB\n\t\tOperator: op_name=\"jit(get_cooccurences)/jit(main)/lt\" source_file=\"/tmp/ipykernel_108300/4272418259.py\" source_line=3\n\t\tXLA Label: fusion\n\t\tShape: pred[1,1,11582]\n\t\t==========================\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mXlaRuntimeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[482], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mget_cooccurences\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkw_paper\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mdevice\n",
      "    \u001b[0;31m[... skipping hidden 10 frame]\u001b[0m\n",
      "File \u001b[0;32m~/conda/envs/3.12/lib/python3.12/site-packages/jax/_src/interpreters/pxla.py:1185\u001b[0m, in \u001b[0;36mExecuteReplicated.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1183\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_token_bufs(result_token_bufs, sharded_runtime_token)\n\u001b[1;32m   1184\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1185\u001b[0m   results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mxla_executable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute_sharded\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_bufs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dispatch\u001b[38;5;241m.\u001b[39mneeds_check_special():\n\u001b[1;32m   1187\u001b[0m   out_arrays \u001b[38;5;241m=\u001b[39m results\u001b[38;5;241m.\u001b[39mdisassemble_into_single_device_arrays()\n",
      "\u001b[0;31mXlaRuntimeError\u001b[0m: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 2817208104 bytes.\nBufferAssignment OOM Debugging.\nBufferAssignment stats:\n             parameter allocation:   135.7KiB\n              constant allocation:         8B\n        maybe_live_out allocation:    8.70MiB\n     preallocated temp allocation:    2.62GiB\n                 total allocation:    2.63GiB\nPeak buffers:\n\tBuffer 1:\n\t\tSize: 511.71MiB\n\t\tOperator: op_name=\"jit(get_cooccurences)/jit(main)/jit(_unique_sorted_mask)/jit(lexsort)/sort[dimension=0 is_stable=True num_keys=2]\" source_file=\"/tmp/ipykernel_108300/2930833792.py\" source_line=3\n\t\tXLA Label: sort\n\t\tShape: s32[134142724]\n\t\t==========================\n\n\tBuffer 2:\n\t\tSize: 511.71MiB\n\t\tOperator: op_name=\"jit(get_cooccurences)/jit(main)/jit(_unique_sorted_mask)/jit(lexsort)/sort[dimension=0 is_stable=True num_keys=2]\" source_file=\"/tmp/ipykernel_108300/2930833792.py\" source_line=3\n\t\tXLA Label: sort\n\t\tShape: s32[134142724]\n\t\t==========================\n\n\tBuffer 3:\n\t\tSize: 511.71MiB\n\t\tOperator: op_name=\"jit(get_cooccurences)/jit(main)/jit(_unique_sorted_mask)/jit(lexsort)/iota[dtype=int32 shape=(134142724,) dimension=0]\" source_file=\"/tmp/ipykernel_108300/2930833792.py\" source_line=3\n\t\tXLA Label: fusion\n\t\tShape: s32[134142724]\n\t\t==========================\n\n\tBuffer 4:\n\t\tSize: 511.71MiB\n\t\tOperator: op_name=\"jit(get_cooccurences)/jit(main)/jit(_unique_sorted_mask)/jit(lexsort)/slice[start_indices=(1, 0) limit_indices=(2, 134142724) strides=(1, 1)]\" source_file=\"/tmp/ipykernel_108300/2930833792.py\" source_line=3\n\t\tXLA Label: fusion\n\t\tShape: s32[1,134142724]\n\t\t==========================\n\n\tBuffer 5:\n\t\tSize: 511.71MiB\n\t\tOperator: op_name=\"jit(get_cooccurences)/jit(main)/jit(_unique_sorted_mask)/jit(lexsort)/slice[start_indices=(1, 0) limit_indices=(2, 134142724) strides=(1, 1)]\" source_file=\"/tmp/ipykernel_108300/2930833792.py\" source_line=3\n\t\tXLA Label: fusion\n\t\tShape: s32[1,134142724]\n\t\t==========================\n\n\tBuffer 6:\n\t\tSize: 127.93MiB\n\t\tOperator: op_name=\"jit(get_cooccurences)/jit(main)/reduce_or[axes=(1,)]\" source_file=\"/tmp/ipykernel_108300/4272418259.py\" source_line=3\n\t\tXLA Label: fusion\n\t\tShape: pred[11582,11582]\n\t\t==========================\n\n\tBuffer 7:\n\t\tSize: 5.80MiB\n\t\tOperator: op_name=\"jit(get_cooccurences)/jit(main)/gather[dimension_numbers=GatherDimensionNumbers(offset_dims=(0, 1), collapsed_slice_dims=(), start_index_map=(0,)) slice_sizes=(760384, 2) unique_indices=True indices_are_sorted=True mode=GatherScatterMode.PROMISE_IN_BOUNDS fill_value=None]\" source_file=\"/tmp/ipykernel_108300/4272418259.py\" source_line=3\n\t\tXLA Label: fusion\n\t\tShape: s32[1,760384,2]\n\t\t==========================\n\n\tBuffer 8:\n\t\tSize: 2.90MiB\n\t\tXLA Label: fusion\n\t\tShape: u32[760384]\n\t\t==========================\n\n\tBuffer 9:\n\t\tSize: 90.5KiB\n\t\tEntry Parameter Subshape: s32[11582,2]\n\t\t==========================\n\n\tBuffer 10:\n\t\tSize: 45.2KiB\n\t\tXLA Label: fusion\n\t\tShape: s32[11582,1]\n\t\t==========================\n\n\tBuffer 11:\n\t\tSize: 45.2KiB\n\t\tOperator: op_name=\"jit(get_cooccurences)/jit(main)/lt\" source_file=\"/tmp/ipykernel_108300/4272418259.py\" source_line=3\n\t\tXLA Label: fusion\n\t\tShape: s32[1,11582]\n\t\t==========================\n\n\tBuffer 12:\n\t\tSize: 45.2KiB\n\t\tOperator: op_name=\"jit(get_cooccurences)/jit(main)/lt\" source_file=\"/tmp/ipykernel_108300/4272418259.py\" source_line=3\n\t\tXLA Label: fusion\n\t\tShape: s32[1,1,11582]\n\t\t==========================\n\n\tBuffer 13:\n\t\tSize: 45.2KiB\n\t\tOperator: op_name=\"jit(get_cooccurences)/jit(main)/lt\" source_file=\"/tmp/ipykernel_108300/4272418259.py\" source_line=3\n\t\tXLA Label: fusion\n\t\tShape: s32[11582,1]\n\t\t==========================\n\n\tBuffer 14:\n\t\tSize: 45.2KiB\n\t\tEntry Parameter Subshape: u32[11582]\n\t\t==========================\n\n\tBuffer 15:\n\t\tSize: 11.3KiB\n\t\tOperator: op_name=\"jit(get_cooccurences)/jit(main)/lt\" source_file=\"/tmp/ipykernel_108300/4272418259.py\" source_line=3\n\t\tXLA Label: fusion\n\t\tShape: pred[1,1,11582]\n\t\t==========================\n\n"
     ]
    }
   ],
   "source": [
    "get_cooccurences(kw_paper).device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "id": "823c4e0b-96d6-4b3f-a70b-6b70f3644a54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[467,  95,  37, ...,   6,   0,   6],\n",
       "       [ 95, 369,  47, ...,   0,   0,   0],\n",
       "       [ 37,  47, 212, ...,   6,   6,   0],\n",
       "       ...,\n",
       "       [  6,   0,   6, ...,  36,   0,   0],\n",
       "       [  0,   0,   6, ...,   0,  36,   0],\n",
       "       [  6,   0,   0, ...,   0,   0,  36]], dtype=uint32)"
      ]
     },
     "execution_count": 457,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cooccurences.todense()["
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "id": "e00193b0-9a86-4840-9414-1474549f3a23",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "save_device_memory_profile() missing 1 required positional argument: 'filename'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[462], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mjax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprofiler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_device_memory_profile\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: save_device_memory_profile() missing 1 required positional argument: 'filename'"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
